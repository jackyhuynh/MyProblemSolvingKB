{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29da3237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before scrap go there and put linkedin/robots.txt\n",
    "# or type this in any website to get their resective rules for web scrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aee32487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html class=\"no-js\" lang=\"en\">\n",
      " <head>\n",
      "  <meta charset=\"utf-8\"/>\n",
      "  <meta content=\"IE=edge\" http-equiv=\"X-UA-Compatible\"/>\n",
      "  <meta content=\"width=device-width, initial-scale=1\" name=\"viewport\"/>\n",
      "  <title>\n",
      "   Truc Huynh\n",
      "  </title>\n",
      "  <link href=\"https://fonts.googleapis.com/css?family=Lato:300,400,700,900\" rel=\"stylesheet\"/>\n",
      "  <style>\n",
      "   /************TAG SELECTOR*************/\n",
      "        h1,h2{\n",
      "            color: red;\n",
      "            font-size: 100px;\n",
      "        }\n",
      "        /************CLASS SELECTOR*************/\n",
      "        .bacon{\n",
      "            background-color: green;\n",
      "        }\n",
      "        .broccoli{\n",
      "            background-color: red;\n",
      "        }\n",
      "  </style>\n",
      " </head>\n",
      " <body>\n",
      "  <h2>\n",
      "   Class selector vs. tags selector\n",
      "  </h2>\n",
      "  <p>\n",
      "   The different between class selector and tags selector\n",
      "  </p>\n",
      "  <ul>\n",
      "   <li>\n",
      "    <div class=\"broccoli\">\n",
      "     Broccoli\n",
      "    </div>\n",
      "   </li>\n",
      "   <li>\n",
      "    <div class=\"bacon\">\n",
      "     Bacon\n",
      "    </div>\n",
      "   </li>\n",
      "  </ul>\n",
      "  <p>\n",
      "  </p>\n",
      " </body>\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "# beautiful soup 4 for web scraping\n",
    "# import lxml\n",
    "\n",
    "with open(\"basic_+_class_selector_vs_tag_+_web.html\", encoding=\"utf8\") as file:\n",
    "    contents = file.read()\n",
    "\n",
    "soup = BeautifulSoup(contents, \"html.parser\")\n",
    "# html.parser or lxml may not work in some websites\n",
    "\n",
    "# print(soup.title)\n",
    "# print(soup.title.name)\n",
    "# print(soup.title.string)\n",
    "# print(soup)\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a17611f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<link href=\"https://fonts.googleapis.com/css?family=Lato:300,400,700,900\" rel=\"stylesheet\"/>]\n",
      "https://fonts.googleapis.com/css?family=Lato:300,400,700,900\n",
      "\n",
      "<h2>\n",
      "            Class selector vs. tags selector\n",
      "        </h2>\n",
      "<div class=\"bacon\">Bacon</div>\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Find the info that we look for using Beautiful Soup\n",
    "\"\"\"\n",
    "\n",
    "all_anchor_tags = soup.find_all(name=\"link\")\n",
    "print(all_anchor_tags)\n",
    "for tag in all_anchor_tags:\n",
    "    print(tag.get(\"href\"))\n",
    "    print(tag.getText())\n",
    "\n",
    "heading = soup.find(name=\"h2\")\n",
    "print(heading)\n",
    "\n",
    "section_heading = soup.find(name=\"div\", class_=\"bacon\")\n",
    "print(section_heading)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2e7833",
   "metadata": {},
   "source": [
    "## Scrap data from live website\n",
    "- Get the most popular topic from https://news.ycombinator.com/\n",
    "- Return the link of the most popular topic and its website\n",
    "- Can use as an attachment to multiple source that can populate blog post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "672e488f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Microsoft Exchange stops passing mail due to bug on 1/1/22 https://old.reddit.com/r/sysadmin/comments/rt91z6/exchange_2019_antimalware_bad_update/\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Get response from the website\n",
    "response = requests.get(\"https://news.ycombinator.com/\")\n",
    "# Store the whole html to the page\n",
    "yc_web_page = response.text\n",
    "\n",
    "# Create the soup to access\n",
    "soup = BeautifulSoup(yc_web_page, \"html.parser\")\n",
    "\n",
    "# find the frst anchor task, with class = title\n",
    "articles = soup.find_all(name=\"a\", class_=\"titlelink\")\n",
    "\n",
    "article_texts = []\n",
    "article_links = []\n",
    "# get the text inside the article_tag\n",
    "for article_tag in articles:\n",
    "    article_texts.append(article_tag.getText())\n",
    "    article_links.append(article_tag.get(\"href\"))\n",
    "\n",
    "article_upvotes = [int(score.getText().split()[0]) for score in soup.find_all(name=\"span\", class_=\"score\")]\n",
    "\n",
    "# print(article_upvotes)\n",
    "# Find the largest index by using the max and largest function\n",
    "largest_index = article_upvotes.index(max(article_upvotes))\n",
    "\n",
    "print(article_texts[largest_index], article_links[largest_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0209aa1c",
   "metadata": {},
   "source": [
    "## Scrap data and publish data to textfile\n",
    "- Example of web scrapping and publishing data to textfile\n",
    "- Data is publish to the current folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ba1f960",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "URL=\"https://web.archive.org/web/20200518073855/https://www.empireonline.com/movies/features/best-movies-2/\"\n",
    "\n",
    "response = requests.get(URL)\n",
    "web_site_html = response.text\n",
    "\n",
    "soup = BeautifulSoup(web_site_html, \"html.parser\")\n",
    "\n",
    "all_movies = soup.find_all(name=\"h3\", class_=\"title\")\n",
    "# print(all_movies)\n",
    "movie_titles = [movie.getText() for movie in all_movies]\n",
    "movies = movie_titles[::-1]\n",
    "\n",
    "with open(\"movies.txt\", mode=\"w\") as file:\n",
    "    for movie in movies:\n",
    "        file.write(f\"{movie}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
